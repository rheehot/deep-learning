# xwMOOC 딥러닝




## 1. 신경망, 딥러닝 팩키지

R에서 신경망, 딥러닝을 돌릴 수 있는 팩키지가 다수 존재한다.
단순한 전방 전달 신경망부터 텐서플로우, MXNet을 활용할 수 있게 하는 팩키지까지 다양하다.

|R 팩키지   |      팩키지 설명                       |
|-----------|----------------------------------------|
|nnet	    | 전방 전달 신경망(feed-forward neural networks) + 다항 로그선형모형(multinomial log-linear models) |
|neuralnet	| 역전파 신경망                          |
|h2o	    | $H_2 O$ 지원                           |
|RSNNS	    | Stuttgart Neural Network Simulator (SNNS) 지원  |
|tensorflow | 구글 텐서플로우                        |
|deepnet	| 딥러닝 툴킷                            |
|darch	    | 볼츠만 기계(Deep Architectures and Restricted Boltzmann Machines)  |
|rnn	    | 순환신경망(Recurrent Neural Networks)  |
|FCNN4R	    | FCNN 라이브러리                        |
|rcppDL	    | 다양한 딥러닝 구현 라이브러리          |
|deepr	    | `darch`, `deepnet` 을 사용하기 쉽게 함 |
|MXNetR	    | `MXNet` 라이브러리                     |

딥러닝에 활용되는 방법론을 중심으로 살펴보면 다음과 같다. 다만 최신 텐서플로우는 포함되어 있지 않다. [^r-deep-learning-comparison]

[^r-deep-learning-comparison]: [Deep Learning in R](http://www.rblog.uni-freiburg.de/2017/02/07/deep-learning-in-r/)

| 팩키지   | 지원되는 신경망 아키텍쳐 |
|----------|---------------------------------------------|
| MXNetR   | Feed-forward neural network, convolutional neural network (CNN)  |
| darch    | Restricted Boltzmann machine, deep belief network                |
| deepnet  | Feed-forward neural network, restricted Boltzmann machine, deep belief network, stacked autoencoders |
| H2O      | Feed-forward neural network, deep autoencoders                   |
| deepr    | 케라스처럼 H2O, deepnet 팩키지를 쉽게 사용할 수 있게 지원        |

### 1.1. 케라스(Keras) [^keras-gura] [^deep-learning-in-r]

[^keras-gura]: [케라스 이야기](https://tykimos.github.io/Keras/2017/01/27/Keras_Talk/)

[^deep-learning-in-r]: [keras: Deep Learning in R](https://www.datacamp.com/community/tutorials/keras-r-deep-learning)

텐서플로우(tensorflow), 티아오(theano), 마이크로소프트 CNTK 등 딥러닝 라이브러리가 너무 많아 어떤 팩키지를 
골라야 하나 고민이 많다. R을 통해 모형을 개발하고 데이터를 분석하는 입장에서는 부족한 시간에 참 고민이 되는 지점이다.
케라스가 다음 4가지 품질속성을 중심으로 딥러닝 모형 개발에 출사표를 던졌다.

- 모듈화 (Modularity)
- 최소주의 (Minimalism)
- 쉬운 확장성
- 파이썬 기반

그런데, 케라스를 R에서 사용할 수 있도록 개발된 팩키지가 Taylor Arnold가 주축이 된 [kerasR](https://github.com/statsmaths/kerasR), 
RStudio에서 개발한 [keras](https://github.com/rstudio/keras) 두가지 버젼이 존재한다.
만약 RStudio 개발 생태계에 익숙하다면 `keras` 팩키지가 시작하기 딱 좋을 수 있다.


### 1.2. 케라스 설치

`kerasR` 보다 `keras` 설치가 단순하다. `devtools` 팩키지로 `install_github`을 통해 `keras`를 설치하고 나서,
`install_tensorflow()`를 실행하면 된다. 다만, 윈도우 사용자의 경우 [아나콘다](https://www.continuum.io/downloads)를 먼저 
설치하고 나서 `install_tensorflow()` 명령어 실행을 추천한다.


~~~{.r}
# 0. 환경설치 -------------------------------------
# devtools::install_github("rstudio/keras")
# library(keras)
# install_tensorflow()

# library(tidyverse)
~~~

## 2. 붓꽃 데이터 분류 

[iris 붓꽃 데이터](http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data)를 예제로 사용해서 시작해보자.
데이터프레임이 R에서 데이터를 분석하기 적합한 자료구조라면, 딥러닝을 시작하는 자료구조는 행렬이다.
따라서, 데이터를 `read_csv` 함수로 불러오면 자동으로 데이터프레임이 된다. 이를 행렬로 변환시킨다.
특히, `dimnames` 함수에 `NULL`값을 넣어 행렬 자료형으로 쉽게 변환하는 기법은 익혀두면 좋다.


~~~{.r}
# 1. 붓꽃 데이터 -------------------------------------
# 1.1. 붓꽃 데이터 가져오기 --------------------------
iris <- read_csv("http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data", col_names = FALSE) 

# 1.2. 데이터프레임을 행렬로 변환 -------------------
iris[,5] <- as.numeric(as.factor(unlist(iris[,5]))) -1
iris <- as.matrix(iris)
dimnames(iris) <- NULL
~~~

## 3. 붓꽃 데이터 딥러닝 시작하기 전 데이터 전처리 

데이터 전처리는 크게 관측점 데이터, 즉 행렬로 봤을 때 $\frac{2}{3}$ 행을 훈련데이터로 사용하고,
$\frac{1}{3}$ 데이터는 검증 데이터로 활용한다. 즉, 개발된 모형의 참성능은 검증데이터 $\frac{1}{3}$을 
활용하여 모형 성능을 검증한다.

피쳐는 정규화과정을 거쳐야 하는데 `keras` 내부에 `normalize()` 함수가 있어 쉽게 정규화할 수 있다.
또한, One-Hot 인코딩기법(통계에서는 가변수, dummy variable)을 통해 범주형 데이터를 0과 1만 갖는 
벡터로 변환한다.


~~~{.r}
# 2. 데이터 전처리 ----------------------------------
## 2.1. 데이터 정규화 -------------------------------
iris_x <- normalize(iris[,1:4])

iris_mat <- cbind(iris_x, iris[,5])
head(iris_mat)
~~~



~~~{.output}
          [,1]      [,2]      [,3]       [,4] [,5]
[1,] 0.8037728 0.5516088 0.2206435 0.03152050    0
[2,] 0.8281329 0.5070201 0.2366094 0.03380134    0
[3,] 0.8053331 0.5483119 0.2227517 0.03426949    0
[4,] 0.8000302 0.5391508 0.2608794 0.03478392    0
[5,] 0.7909650 0.5694948 0.2214702 0.03163860    0
[6,] 0.7841750 0.5663486 0.2468699 0.05808704    0

~~~



~~~{.r}
# 3. 딥러닝 모형 -----------------------------------
## 3.1. 훈련표본과 검증표본 ------------------------- 
ind <- sample(2, nrow(iris_mat), replace=TRUE, prob=c(0.67, 0.33))

### 모형 설계행렬
iris.training <- iris_mat[ind==1, 1:4]
iris.test <- iris_mat[ind==2, 1:4]

### 모형 예측변수
iris.trainingtarget <- iris_mat[ind==1, 5]
iris.testtarget <- iris_mat[ind==2, 5]

### One-Hot 인코딩: 훈련예측변수
iris.trainLabels <- to_categorical(iris.trainingtarget)

### One-Hot 인코딩: 검증예측변수
iris.testLabels <- to_categorical(iris.testtarget)
~~~

## 4. 딥러닝 모형 적합

딥러닝 모형을 개발하려면 데이터를 가져오고, 데이터를 전처리하고 나서, 데이터에 적합한 
딥러닝 모형을 적용해야 하는데 신경망 계층(layer)은 몇층으로 할지, 노드는 몇개로 할지,
활성화(activation) 함수는 무엇으로 할지, 하이퍼 모수 학습률(learning rate)은 어떻게 정할지,
다양한 조합이 모형의 성능에 영향을 미치게 된다. 그런 점에서 케라스는 모형자체에 
개발자가 집중할 수 있도록 함으로써 큰 도움을 주고 있다.

모형을 `keras_model_sequential()`로 초기화하고 나서,
"Sepal.Length", "Sepal.Width", "Petal.Length", "Petal.Width" 4개가 입력으로 들어가도록 설정한다.
`input_shape = c(4)`가 역할을 하고 계층은 입력계층 하나, 출력계층 하나 총 두개로 놓는데,
`layer_dense`를 통해 해결하고 `layer_dropout`을 사이에 넣어 신경망 모형을 단순화한다.
그리고 활성화함수는 `relu`, `softmax`를 지정한다.

모형을 적합시키기 전에 `compile` 단계를 지정하는데 신경망 아키텍처가 지정된 후에,
손실(loss), 최적화, 측도(metrics)를 지정한다.

그리고 나서 `fit` 단계에서 모형을 학습시킨다. `epoch`은 몇회를 할지 배치크기는 얼마로 할지,
검증(validation)은 어떻게 나눌지 등등을 지정하여 학습을 돌린다.



~~~{.r}
## 3.2. 모형 개발 ------------------------- 
set.seed(777)
### 3.2.1. 모형 초기화
model <- keras_model_sequential()

### 3.2.2. 모형에 계층 추가

# 4 inputs -> [8 hidden nodes] -> 3 outputs
model %>% 
    layer_dense(units = 8, activation = 'relu', input_shape = c(4)) %>% 
    layer_dropout(rate = 0.5) %>% 
    layer_dense(units = 3, activation = 'softmax')

### 3.2.3. 모형 살펴보기
summary(model)
~~~



~~~{.output}
Model
___________________________________________________________________________
Layer (type)                     Output Shape                  Param #     
===========================================================================
dense_1 (Dense)                  (None, 8)                     40          
___________________________________________________________________________
dropout_1 (Dropout)              (None, 8)                     0           
___________________________________________________________________________
dense_2 (Dense)                  (None, 3)                     27          
===========================================================================
Total params: 67
Trainable params: 67
Non-trainable params: 0
___________________________________________________________________________

 

~~~



~~~{.r}
# get_config(model)
# get_layer(model, index = 1)
# model$layers
# model$inputs
# model$outputs

### 3.2.4. 모형 컴파일
model %>% compile(
    loss = 'categorical_crossentropy',
    optimizer = 'adam',
    metrics = 'accuracy'
)

### 3.2.5. 모형 적합
model %>% fit(
    iris.training, 
    iris.trainLabels, 
    epochs = 500, 
    batch_size = 5,
    validation_split = 0.1
)
~~~

## 5. 딥러닝 모형 적합성 시각화

딥러닝 모형이 잘 학습되었는지, 즉 과대학습 혹은 과소학습이 되지 않았나 
훈련표본과 검증표본의 손실과 정확도 두가지 측면에서 시각적으로 확인한다.



~~~{.r}
### 3.2.6. 적합된 모형 시각화
history <- model %>% fit(
    iris.training, 
    iris.trainLabels, 
    epochs = 500,
    batch_size = 5,
    validation_split = 0.1
)

listviewer::jsonedit(history, model="view")
~~~

<!--html_preserve--><div id="htmlwidget-b07e58feb102582cf33f" style="width:672px;height:480px;" class="jsonedit html-widget"></div>
<script type="application/json" data-for="htmlwidget-b07e58feb102582cf33f">{"x":{"data":{"params":{"batch_size":5,"epochs":500,"samples":97,"verbose":1,"do_validation":true,"metrics":["loss","acc","val_loss","val_acc"],"validation_samples":11},"metrics":{"val_loss":[0.574020656672391,0.561404740268534,0.535718963904814,0.508024958047,0.510834767059846,0.530998202887448,0.51808589425954,0.535998940467834,0.549481459639289,0.548059650442817,0.569861271164634,0.546587673100558,0.522771735082973,0.532120970162478,0.500150832262906,0.509106481617147,0.509390928528526,0.544034044850956,0.55123896761374,0.525237798690796,0.538700057701631,0.548258272084323,0.542239235206084,0.529427555474368,0.528110273859718,0.559571986848658,0.532926274971528,0.522546123374592,0.529439292170785,0.5210826207291,0.536998158151453,0.539502086964521,0.545821566473354,0.555921294472434,0.557979938658801,0.549259814349088,0.583962830630216,0.565366073088212,0.56045570156791,0.531893960454247,0.521960632367568,0.551945366642692,0.570630964907733,0.538628258488395,0.507731608369134,0.467382393100045,0.482195716012608,0.516646444797516,0.535245925188065,0.509707719087601,0.516663017598065,0.503380493684249,0.517983989282088,0.539396464824677,0.508687366138805,0.484278318556872,0.520884662866592,0.536861804398623,0.531948826529763,0.52565502578562,0.573542107235302,0.557660698890686,0.527664596384222,0.560114072127776,0.592502268877896,0.550358642231334,0.52419695800001,0.499442978338762,0.503437594933943,0.513974796641957,0.517291499809785,0.509842593561519,0.528880932114341,0.536240363662893,0.532503667202863,0.51133364980871,0.499265245415948,0.494489978660237,0.486770524220033,0.490340211174705,0.494093957272443,0.55073927749287,0.508157161149112,0.505400197072463,0.512673093514009,0.522544164549221,0.516974007541483,0.501767394217578,0.520501321012324,0.521999952468005,0.528880430893464,0.527118921279907,0.524397695606405,0.510360985994339,0.499194605783983,0.47890920530666,0.469370021061464,0.471362219615416,0.484128607945009,0.493811098012057,0.470561637119813,0.475140176036141,0.458818143064326,0.505717610771006,0.51908829537305,0.524391941048882,0.528748084198345,0.527273589914495,0.539424660530957,0.510220600800081,0.497852298346433,0.485414987260645,0.471366933800957,0.494892098686912,0.5048520619219,0.525393751534549,0.494892166419463,0.484942631288008,0.475587235255675,0.500896884636445,0.533658780834892,0.536282409321178,0.539030700922012,0.531591775742444,0.51951836455952,0.515372894027016,0.537970675663515,0.552989220077341,0.540241954001513,0.532966933467171,0.508133720267903,0.483050877397711,0.495809145949104,0.496051983399825,0.487067555839365,0.505344477566806,0.512535623528741,0.496674453670328,0.52520657398484,0.500756648453799,0.495761229233308,0.508933912624012,0.528192086653276,0.51927846941081,0.486294803294268,0.484838309613141,0.474440997297114,0.481128703464161,0.474994643168016,0.461024048653516,0.464996026320891,0.480692782185294,0.505109410394322,0.511943139813163,0.489185476844961,0.49231114170768,0.506385578350587,0.504920973019166,0.496667748147791,0.488540161739696,0.483233427459543,0.506642309102145,0.503641711039977,0.47872194647789,0.470155816186558,0.47518373077566,0.487749056382613,0.491064895283092,0.488617357882586,0.464353759180416,0.455221571705558,0.445201600139791,0.458155949007381,0.480965226888657,0.469270194118673,0.459839108315381,0.493234750899402,0.493592991070314,0.486593774773858,0.492731554941698,0.498805522918701,0.502621461044658,0.479232834144072,0.456572139805013,0.447745125402104,0.46434580466964,0.46655998988585,0.496290347792886,0.471687319603833,0.473943041129546,0.484510554508729,0.499228314919905,0.487304915081371,0.483639194206758,0.475770668549971,0.472140233625065,0.474328675053336,0.484862926331433,0.480588950894096,0.475232349200682,0.479483609849756,0.498311015692624,0.493199985135685,0.480491543358023,0.472969217733903,0.488611779429696,0.478562609715895,0.472481995820999,0.479293416846882,0.473930716514587,0.464419150894338,0.460056015036323,0.438484487208453,0.435955529863184,0.43519816886295,0.450830332257531,0.45428589257327,0.454923017458482,0.455845800313083,0.441596088084308,0.451789620247754,0.444272436878898,0.457822666926817,0.450017503716729,0.461980426853353,0.457422741434791,0.447337028655139,0.464624052697962,0.452566642652858,0.45196597142653,0.48855339938944,0.513421167026867,0.492627336220308,0.47593967481093,0.505522841756994,0.484161712906577,0.485075034878471,0.474555543877862,0.485882485454733,0.482185840606689,0.472544038837606,0.449587518518621,0.455542881380428,0.469011163169687,0.489386607300151,0.491190013560382,0.493454567410729,0.492386541583321,0.45757193998857,0.463902630589225,0.461425724354657,0.462317930026488,0.481616339900277,0.516103844751011,0.503549676049839,0.481260177764026,0.494861941445958,0.489994133060629,0.481024417010221,0.475464891303669,0.44635085626082,0.446849736300382,0.445414594628594,0.464574480598623,0.456176280975342,0.444135554812171,0.465186766602776,0.446743520823392,0.454941654747183,0.439896166324615,0.446887357668443,0.44618634744124,0.470890026200901,0.468792316588488,0.465960632671009,0.462851611050692,0.470553793690421,0.459183218804273,0.465630108659918,0.481871268965981,0.501572319052436,0.462606096809561,0.455430981787768,0.445657960393212,0.452316595749422,0.4404990429228,0.447427272796631,0.439871907234192,0.439840969714251,0.452976644039154,0.460369145328348,0.450591157783162,0.457368192347613,0.463450662114403,0.47042959115722,0.464273157444867,0.468977781859311,0.489507192915136,0.491859509186311,0.495313270525499,0.509350863370028,0.485686245289716,0.47716068408706,0.47293685241179,0.456367443908345,0.455619289116426,0.456474361094561,0.46001966974952,0.474716554988514,0.458588369868018,0.451731129126115,0.446633265777068,0.454701315272938,0.467880920930342,0.446032361550765,0.46191471544179,0.459977033463391,0.477312302047556,0.47078291665424,0.457963997667486,0.437825479290702,0.468649089336395,0.441057600758292,0.422011562369087,0.431982649998231,0.425454787232659,0.4483723775907,0.446676939725876,0.415945299647071,0.421286796981638,0.421061504970897,0.443172397938642,0.438472506674853,0.441442340612411,0.443378719416532,0.451603670011867,0.445549203590913,0.461528252471577,0.439659180966291,0.43596355210651,0.452663966200568,0.44228659434752,0.448332407257774,0.435601096261631,0.451491924849424,0.463825426318429,0.453978514129465,0.485804923556068,0.476086627353321,0.444008588790894,0.442305621775714,0.452838512984189,0.453987381675027,0.432669883424586,0.439595187252218,0.459680839018388,0.437386694279584,0.438307247378609,0.448250328952616,0.450361793691462,0.44689983942292,0.455600107258016,0.435930615121668,0.438426394354213,0.438734149391001,0.448848548260602,0.448943969878283,0.431354333053936,0.44570300524885,0.476392556320537,0.482999373566021,0.460823050954125,0.445266899737445,0.445257181471044,0.42978970841928,0.440751525488767,0.431014868346128,0.443452986803922,0.454063550992445,0.446138847957958,0.43198238177733,0.445754888382825,0.436185316606001,0.431683193553578,0.443325137550181,0.433931743556803,0.429571165279909,0.432280180129138,0.438463113524697,0.453688098625703,0.468193558129397,0.451918984001333,0.446902681480755,0.460609861395576,0.448890574953773,0.413368775085969,0.414420168508183,0.415555506944656,0.409687521782788,0.412335463545539,0.41334683786739,0.428503602743149,0.463822895830328,0.454289254817096,0.460326313972473,0.431736130606044,0.435501662167636,0.431506568735296,0.422978924079375,0.42441683194854,0.421712867238305,0.443816225637089,0.434177314693278,0.423924600536173,0.413906330412084,0.397189999168569,0.395162517374212,0.409875344146382,0.414137872782621,0.415910666639155,0.429185753518885,0.438378531824459,0.409229454669085,0.421020610765977,0.429944217205048,0.430614750493656,0.426682461391796,0.435273013331673,0.44217477332462,0.46825723214583,0.455553604797883,0.430699519135735,0.422932922840118,0.429803092371334,0.449522828513926,0.433265507221222,0.430297393690456,0.429181109775196,0.430448678406802,0.438675140792673,0.435212165117264,0.445722103118896,0.434330495921048,0.44694966619665,0.434099213643508,0.412916435436769,0.420213436538523,0.419974668459459,0.423900620503859,0.423841002312574,0.441000217741186,0.426878227428956,0.429511652751402,0.41432768648321,0.41678340326656,0.392107519236478,0.397656205025586,0.400167776779695,0.403458551927046,0.395513198592446,0.401166447184303,0.406210977922786,0.412422716617584,0.40744391896508,0.396023389968005,0.368893980979919,0.36095945130695,0.388140510429036,0.387272021987221,0.388227479024367,0.367337812076915,0.368763051249764,0.37072307955135,0.367041652852839,0.371383859352632,0.378324701027437,0.363213116472418,0.372795993631536,0.396391147916967,0.378914085301486,0.385785230181434,0.412895609032024,0.401409149169922,0.38421676104719,0.401815300638025,0.393386363983154,0.40140651030974,0.39873716506091,0.418834586035122,0.413343692367727,0.409394860267639,0.399191005663438,0.402174543250691,0.401839025995948,0.401541587981311,0.404254894364964,0.41256785121831,0.392460663210262,0.383639560504393,0.396238484165885],"val_acc":[0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.909090914509513,0.909090914509513,0.909090914509513,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.909090914509513,0.909090914509513,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.909090914509513,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.909090914509513,0.909090914509513,0.909090914509513,0.818181829019026,0.909090914509513,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.909090914509513,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.818181829019026,0.909090914509513,0.909090914509513,0.909090914509513,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.909090914509513,0.909090914509513,0.909090914509513,0.818181829019026,0.818181829019026,0.818181829019026,0.909090914509513,0.909090914509513,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.909090914509513,0.909090914509513,0.818181829019026,0.818181829019026,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.818181829019026,0.818181829019026,0.909090914509513,0.818181829019026,0.818181829019026,0.818181829019026,0.909090914509513,0.909090914509513,0.909090914509513,0.818181829019026,0.818181829019026,0.909090914509513,0.909090914509513,0.909090914509513,0.818181829019026,0.818181829019026,0.818181829019026,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.818181829019026,0.818181829019026,0.909090914509513,0.818181829019026,0.818181829019026,0.818181829019026,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.818181829019026,0.818181829019026,0.909090914509513,0.909090914509513,0.909090914509513,0.818181829019026,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.818181829019026,0.818181829019026,0.818181829019026,0.909090914509513,0.818181829019026,0.909090914509513,0.909090914509513,0.909090914509513,0.818181829019026,0.909090914509513,0.818181829019026,0.818181829019026,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.818181829019026,0.909090914509513,0.818181829019026,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.818181829019026,0.818181829019026,0.909090914509513,0.909090914509513,0.818181829019026,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.818181829019026,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.818181829019026,0.818181829019026,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.818181829019026,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.818181829019026,0.909090914509513,0.909090914509513,0.818181829019026,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.818181829019026,0.818181829019026,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,1,1,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513,0.909090914509513],"loss":[0.375244805493306,0.409655072025417,0.338772927324489,0.390339402808357,0.39433131236391,0.350864931480172,0.413348952980386,0.389377351581436,0.387263459643138,0.4375275328909,0.368924514995408,0.424132503492316,0.324462727787568,0.365612614823064,0.424905106578906,0.427925457429026,0.397250597163574,0.311816177225297,0.445858333067796,0.350938731401237,0.334638239489388,0.486395339606349,0.378754531907052,0.381634531139406,0.45134605812965,0.424849428774155,0.361088239232597,0.363305150310403,0.450461407796931,0.372319011651364,0.336118655490507,0.378310374301119,0.339338051596868,0.349096784886626,0.36080970565222,0.344671530130598,0.362515647156337,0.354043011659199,0.329713297182137,0.348988791274809,0.411388778041319,0.404929680538546,0.378507358204458,0.385493871170221,0.408970874839837,0.318633181379013,0.394411706663284,0.353603480710197,0.374181566932767,0.420134359490626,0.357365303471224,0.352209825952028,0.43839111981779,0.358330122934482,0.416081626774724,0.377781837103293,0.424198283977115,0.342806339763182,0.378431223947363,0.344019952662212,0.359689640937392,0.440792274728571,0.373016780799197,0.356088341050541,0.322288225990595,0.361279208846774,0.316429194946264,0.372336845806579,0.327551923615416,0.360964114134459,0.383152499372504,0.382691752050341,0.334854505264882,0.38166579711683,0.320208540128678,0.331962040788734,0.372634242394382,0.397876224912626,0.400467902882812,0.357980886378239,0.433043199946585,0.361774347952961,0.398640218853336,0.376470120604505,0.424039492791653,0.427676471468714,0.40751776091524,0.34314967415382,0.357289294453011,0.413210750317451,0.395102226703437,0.415655646281144,0.322987546549015,0.437234504474807,0.343753463367826,0.373625227086937,0.400495548185307,0.436934250915788,0.289337635462739,0.37138252298242,0.406119535725141,0.280219006016082,0.375435923024551,0.281491128439756,0.356031072758061,0.400276776825644,0.444157356377115,0.382854880861093,0.272651301293643,0.354394173115185,0.392826104536653,0.353962121742595,0.33358258427572,0.348429672804075,0.426270825423531,0.420303817646405,0.354231406425692,0.364766086192475,0.36454891609316,0.487787805881697,0.359678125258574,0.414720783989454,0.340029515710074,0.310210685524129,0.436027521248331,0.418996551448537,0.415637328836721,0.361646604368982,0.297106938417425,0.355828878152923,0.359198096217876,0.307081247497465,0.310666190863577,0.347983478578096,0.362292711037336,0.346454606151458,0.35911243831374,0.362695404856475,0.346081875802315,0.381834546238491,0.393871194750224,0.367287847645504,0.405196834624428,0.399036386424733,0.347651812987229,0.364902915337037,0.375801942551259,0.280801297309472,0.362046436473881,0.396821764941068,0.308226362638867,0.383941232620441,0.387742722695021,0.406423135939025,0.350300416734415,0.33509301263647,0.396095257866936,0.393170000841246,0.37478136071532,0.32416806439149,0.350469574063402,0.324697107921556,0.362215452433861,0.277671820800943,0.344556635956174,0.352610923426667,0.306795932218125,0.39640504513512,0.330137343289926,0.387224665929362,0.380079659297294,0.374707258700096,0.34319754052408,0.330709046405769,0.334424146211024,0.339860488689437,0.30923913665039,0.423872641143725,0.408124595880508,0.318378528215222,0.333457442595787,0.347379630297115,0.32388479646641,0.276406627993301,0.326312673038121,0.385920132255931,0.348242379302524,0.418232189779429,0.352532711571332,0.339430635430149,0.348625050118366,0.352957571328608,0.310852089954406,0.348638411649724,0.437352709241749,0.413165558321574,0.343200541802288,0.360345599270359,0.432287369806742,0.30865054896351,0.451165376864758,0.3437394395317,0.396991673818569,0.31712534140373,0.291206400188589,0.356673867278492,0.282211437763781,0.343571364687583,0.378151714878598,0.354210562619966,0.346099142100393,0.314812939057184,0.418041461982678,0.356386683189992,0.372262493284784,0.36764668719363,0.333515651370447,0.302147332079632,0.375993333680114,0.345222283884422,0.337079057258736,0.343456635117224,0.314521882068558,0.307570133819101,0.348381178434362,0.439038668442325,0.385787276032659,0.372006171834223,0.369648353379105,0.331501706052072,0.303296052994802,0.394672716938958,0.363833395092143,0.352842198780824,0.313588838793875,0.372478236780339,0.409380107388361,0.377644615612694,0.395236463353191,0.357954839455713,0.391870644673244,0.318363005852269,0.339149474483175,0.417750758531782,0.285090045369778,0.314778913657383,0.335593898041347,0.411082318302282,0.279164563442014,0.376670931571538,0.327506631881613,0.319634613532995,0.410177387834824,0.344682439624034,0.277443738289408,0.412458492154928,0.478975439348172,0.323308582081623,0.378257526411224,0.306893540200653,0.365678812886022,0.26146778971264,0.399448765614598,0.327106380892783,0.429935529136781,0.35551439820952,0.417107275235899,0.318008293196098,0.314895064406788,0.449042431011642,0.371901877838926,0.348523131158856,0.342640515532076,0.378775001493926,0.371024399726968,0.344239541473462,0.364336990664915,0.315024000237283,0.323287178138175,0.311095548121585,0.294516523627891,0.322823714474504,0.253647303986411,0.295966694131494,0.324895697194584,0.404685280372187,0.4016611702817,0.313560897081169,0.388329237689907,0.363758757748862,0.307780823065448,0.37340444487702,0.380999982971506,0.298674048606268,0.349416275575911,0.328575401197274,0.40567748202491,0.379472509948249,0.420047479576057,0.372677720468683,0.306860118643525,0.356878467825885,0.340629031478437,0.298364082485745,0.376422884077141,0.362796820200903,0.333279836385213,0.368438423179167,0.33880779373738,0.351443207179455,0.328313503107152,0.35652438524304,0.357338247532697,0.379798029655034,0.290770133573216,0.382501740354238,0.31272441734435,0.327459729478224,0.344470422753354,0.332657054757949,0.317978352884349,0.33298910355445,0.329312465240046,0.278300407313809,0.250269678718958,0.391263022020306,0.298237636531751,0.311194291011882,0.362471722911314,0.318030881436215,0.367528706481776,0.33034693011918,0.230536515373237,0.391566465656782,0.32862927634077,0.340828547542243,0.341885831168632,0.310688828531,0.464508639178095,0.382109818211997,0.28238283470273,0.301834373917315,0.379328711207994,0.347799242963803,0.321585355845002,0.393337317041515,0.334921401378113,0.435227414074632,0.363809580578632,0.388543157694266,0.353093288109167,0.335539648789413,0.398492095233469,0.258036840476633,0.329141074301886,0.379189360118711,0.389554021207942,0.352622779335865,0.324367985052546,0.355946712810354,0.339482956748341,0.352102373448229,0.387547474700151,0.355383853706502,0.296236220670422,0.339581029547244,0.291879849527607,0.399984928212829,0.349422455986136,0.380618234385842,0.379816291988203,0.469417177217523,0.284474306499835,0.363184764443599,0.318352897065816,0.388578517381678,0.465618180245468,0.343055477692294,0.326785614272368,0.313445771554696,0.346839447880221,0.27104879017036,0.350297089090052,0.307815256874202,0.370599252123808,0.326086608174535,0.273836696194005,0.400667752571327,0.288555698757319,0.38569626530882,0.36046857087268,0.266208980040452,0.269172694831703,0.319500520306918,0.32588071145655,0.354134010688699,0.399319339697201,0.370793679402661,0.412769483959245,0.305200760608021,0.308817039829554,0.454821620558955,0.443920727857609,0.392724983778197,0.369809289530073,0.462660720821508,0.300344444580914,0.348462361909603,0.319586573955939,0.380585577588567,0.326617928145012,0.387292127019351,0.302528232566475,0.367793591750651,0.313462769497455,0.408731622333379,0.296151878907508,0.430058040377713,0.39472501425399,0.320623354199006,0.367492741377083,0.326560481537863,0.428540413507942,0.376000105566585,0.362714749367274,0.338607037958411,0.385154136139707,0.308720883250851,0.339339183969903,0.30387847663201,0.288541090933932,0.34277205937302,0.409759536596764,0.373418015994362,0.373676882423076,0.34117954185943,0.271871342909398,0.343252334858953,0.325401949859464,0.299129561281081,0.354425550448065,0.37903202532493,0.316598447020521,0.371069705132971,0.32508399975054,0.34064958121666,0.305653425682451,0.413596580054649,0.354263400178902,0.40065859160242,0.295175771814646,0.307515301885679,0.259151141898534,0.309736753631498,0.299960021467246,0.303502011237685,0.309235330474246,0.42608523418762,0.362580909403329,0.426188076587067,0.344100914511484,0.277424291006683,0.315668354943856,0.270558846634381,0.336353010008323,0.308461589489094,0.343680308344438,0.409231729978292,0.254371991880315,0.326972487430597,0.312822011020995,0.250804872492079,0.289109772954559,0.334045906272746,0.28227275890327,0.305039539549154,0.40763355246217,0.327500303765548,0.392182248846157,0.339477169221824,0.309841601974955,0.300435547515289,0.31289266787239,0.391569599278809,0.296950997842341,0.353517328417793,0.366860161199398,0.344161807088969,0.349536464309569,0.362555708006485,0.365468217616843,0.321495604576524,0.333358545509196,0.284184045167928,0.300098991175134,0.335680034357248,0.278353698839693,0.405110082675501,0.379778571542084,0.354619077646855],"acc":[0.82474227695121,0.793814442206904,0.835051553151042,0.845360832423279,0.835051555608966,0.86597938943155,0.804123721479141,0.804123721479141,0.814432997678973,0.835051556223447,0.835051553919143,0.773195884430531,0.865979387895348,0.855670110159314,0.814432997678973,0.80412372071104,0.865979388817069,0.855670111695516,0.762886611456724,0.804123722400862,0.865979387895348,0.7628866089988,0.835051554687244,0.835051554687244,0.793814444664827,0.711340218167944,0.845360832423279,0.824742278487412,0.752577333720689,0.804123721479141,0.876288667167585,0.845360832423279,0.814432999215175,0.855670111695516,0.835051552536561,0.835051555608966,0.814432997678973,0.835051556223447,0.876288667167585,0.835051553151042,0.793814442206904,0.793814442975005,0.814433000136896,0.845360833959481,0.835051555608966,0.876288667167585,0.82474227695121,0.896907222639654,0.845360833959481,0.814433000136896,0.835051556223447,0.865979387895348,0.804123721479141,0.824742278487412,0.876288667167585,0.814432997064492,0.762886610535002,0.865979388817069,0.876288665016902,0.824742278487412,0.835051554687244,0.783505163702768,0.845360831808798,0.824742277872931,0.876288667167585,0.845360833959481,0.86597938943155,0.845360833959481,0.86597938943155,0.855670109544833,0.845360833959481,0.876288666553104,0.876288667167585,0.855670111081035,0.855670110159314,0.88659794490362,0.835051554687244,0.845360833959481,0.82474227695121,0.845360832423279,0.793814443128625,0.855670111081035,0.814432999215175,0.865979387895348,0.82474227695121,0.804123718406736,0.814432997678973,0.845360832423279,0.845360833959481,0.773195888271037,0.845360833959481,0.804123723015343,0.86597938943155,0.814433000751377,0.876288666553104,0.824742275415008,0.80412372086466,0.845360833959481,0.907216500375689,0.896907222639654,0.855670111695516,0.907216499761208,0.865979388817069,0.876288666553104,0.845360831808798,0.845360833959481,0.804123721479141,0.845360833345,0.896907222639654,0.835051553151042,0.845360833959481,0.876288665016902,0.86597938943155,0.845360833959481,0.814432997678973,0.814433000751377,0.855670111081035,0.865979387280867,0.835051553151042,0.773195887042075,0.82474227695121,0.793814443743106,0.86597938943155,0.835051552382941,0.824742276336729,0.814432998600694,0.762886609920521,0.804123721479141,0.845360833345,0.824742275415008,0.86597938943155,0.907216500375689,0.896907221103452,0.814432996142771,0.824742277872931,0.835051554687244,0.855670111695516,0.835051556223447,0.855670108930352,0.86597938943155,0.835051554687244,0.81443299552829,0.793814443743106,0.82474227695121,0.88659794490362,0.876288667167585,0.855670111695516,0.865979386512766,0.886597944289139,0.835051556223447,0.896907222639654,0.814432999215175,0.835051556223447,0.845360833959481,0.865979387280867,0.855670109391212,0.793814442206904,0.855670108623111,0.835051554072764,0.876288667167585,0.845360832730519,0.88659794490362,0.80412372086466,0.896907222639654,0.824742278487412,0.876288667167585,0.88659794490362,0.814432998600694,0.865979386359146,0.835051556223447,0.824742276336729,0.845360833959481,0.845360831194317,0.876288665631383,0.896907222025173,0.907216500375689,0.907216500375689,0.773195886734835,0.793814445279308,0.845360833959481,0.855670111695516,0.845360832423279,0.876288667167585,0.917525778111723,0.886597944289139,0.855670111695516,0.886597944289139,0.82474227695121,0.835051554072764,0.865979388817069,0.835051553151042,0.865979387895348,0.88659794490362,0.845360830887077,0.773195886734835,0.855670111695516,0.814433000751377,0.835051556223447,0.82474227695121,0.896907220488971,0.783505163088287,0.845360833959481,0.845360833959481,0.876288667167585,0.886597944289139,0.855670111081035,0.917525776575521,0.865979388817069,0.86597938943155,0.845360832423279,0.845360833345,0.865979387895348,0.845360831808798,0.876288665631383,0.835051554687244,0.855670109391212,0.876288667167585,0.907216499761208,0.824742277872931,0.865979388817069,0.86597938943155,0.865979388817069,0.876288667167585,0.876288667167585,0.845360831655178,0.824742278487412,0.835051555608966,0.824742277872931,0.824742278487412,0.88659794490362,0.88659794490362,0.814433000751377,0.855670111695516,0.82474227695121,0.86597938943155,0.824742278487412,0.793814443743106,0.835051554994485,0.835051556223447,0.86597938943155,0.835051556223447,0.865979388817069,0.86597938943155,0.824742274800527,0.88659794490362,0.835051553151042,0.82474227695121,0.773195887656556,0.917525776575521,0.88659794490362,0.876288667167585,0.886597944289139,0.804123721479141,0.855670111695516,0.917525778111723,0.835051554687244,0.752577332184487,0.845360832423279,0.824742275722248,0.876288667167585,0.88659794490362,0.896907222639654,0.865979387895348,0.855670111695516,0.855670110466554,0.876288667167585,0.855670111695516,0.88659794490362,0.855670110159314,0.804123722400862,0.845360832423279,0.835051556223447,0.855670110159314,0.845360833959481,0.804123722400862,0.845360832423279,0.835051556223447,0.845360833959481,0.876288667167585,0.88659794490362,0.907216500375689,0.855670109391212,0.907216500375689,0.88659794490362,0.855670108623111,0.845360832423279,0.814432997064492,0.896907222639654,0.855670110159314,0.814432997678973,0.88659794490362,0.82474227695121,0.865979387895348,0.907216500375689,0.876288666553104,0.86597938943155,0.773195886120354,0.824742276183109,0.845360831808798,0.835051556223447,0.876288667167585,0.845360833959481,0.855670110159314,0.855670109544833,0.855670109544833,0.835051556223447,0.82474227695121,0.82474227695121,0.855670110159314,0.88659794490362,0.876288665631383,0.835051556223447,0.865979388817069,0.793814443743106,0.896907222025173,0.814433000751377,0.86597938943155,0.865979387895348,0.835051556223447,0.88659794490362,0.86597938943155,0.86597938943155,0.896907222639654,0.886597944289139,0.927835055233277,0.814432998600694,0.88659794490362,0.88659794490362,0.845360830887077,0.855670109544833,0.824742278487412,0.876288666553104,0.917525776575521,0.80412372086466,0.86597938943155,0.835051555608966,0.86597938943155,0.88659794490362,0.793814443743106,0.783505166007071,0.886597943367417,0.876288666553104,0.855670109544833,0.855670108623111,0.88659794490362,0.804123719942938,0.855670111695516,0.804123719942938,0.835051556223447,0.82474227695121,0.814433000751377,0.855670111695516,0.814432999215175,0.917525778111723,0.886597944289139,0.80412372086466,0.86597938943155,0.855670108623111,0.845360833959481,0.86597938943155,0.876288667167585,0.855670111695516,0.845360830118976,0.855670111695516,0.907216500375689,0.855670111695516,0.88659794490362,0.835051552536561,0.865979388817069,0.824742278487412,0.835051556223447,0.814433000751377,0.86597938943155,0.855670110159314,0.896907222639654,0.835051554072764,0.773195889192758,0.855670109391212,0.876288667167585,0.876288667167585,0.845360833959481,0.896907222639654,0.855670108623111,0.876288667167585,0.845360833345,0.876288667167585,0.88659794490362,0.814433000136896,0.845360833959481,0.824742278487412,0.855670110159314,0.865979387895348,0.88659794490362,0.88659794490362,0.865979388817069,0.876288667167585,0.835051554994485,0.845360833345,0.814432996910872,0.865979388817069,0.907216499761208,0.804123722400862,0.835051554072764,0.855670111695516,0.824742275415008,0.814433000751377,0.865979387895348,0.855670111695516,0.876288667167585,0.86597938943155,0.824742278487412,0.804123721479141,0.86597938943155,0.845360831808798,0.876288666553104,0.835051554687244,0.876288667167585,0.814432999215175,0.835051556223447,0.876288667167585,0.845360831808798,0.88659794490362,0.783505166007071,0.835051555608966,0.86597938943155,0.845360833345,0.835051554687244,0.896907222639654,0.855670111081035,0.855670111695516,0.876288665631383,0.896907222639654,0.793814445279308,0.855670108776732,0.855670111695516,0.86597938943155,0.86597938943155,0.845360833959481,0.855670111695516,0.845360833959481,0.865979387895348,0.835051554072764,0.855670111695516,0.855670109544833,0.845360833959481,0.88659794490362,0.886597943367417,0.824742277872931,0.907216500375689,0.804123723015343,0.855670111695516,0.917525778111723,0.855670111695516,0.876288667167585,0.876288667167585,0.845360831808798,0.865979386359146,0.845360832423279,0.845360832423279,0.814433000751377,0.855670110159314,0.948453611319827,0.886597944289139,0.88659794490362,0.88659794490362,0.845360832423279,0.845360833959481,0.82474227695121,0.917525778111723,0.86597938943155,0.855670111695516,0.886597943367417,0.865979386359146,0.86597938943155,0.876288667167585,0.896907222639654,0.855670111695516,0.86597938943155,0.804123718560356,0.86597938943155,0.845360833959481,0.876288667167585,0.835051556223447,0.845360833345,0.88659794490362,0.88659794490362,0.835051556223447,0.865979387895348,0.835051556223447,0.835051555608966,0.845360830272596,0.876288667167585,0.865979388817069,0.907216500375689,0.865979387895348,0.876288667167585,0.876288667167585,0.804123718406736,0.86597938943155,0.855670109391212]}},"options":{"mode":"tree","modes":["code","form","text","tree","view"],"model":"view"}},"evals":[],"jsHooks":[]}</script><!--/html_preserve-->

~~~{.r}
### 모형 수렴
plot(history$metrics$loss, main="Model Loss", xlab = "epoch", ylab="loss", col="blue", type="l",
     ylim=c(0,1))
lines(history$metrics$val_loss, col="green")
legend("topright", c("train","test"), col=c("blue", "green"), lty=c(1,1))
~~~

<img src="fig/keras-iris-deep-learning-fit-2.png" style="display: block; margin: auto;" />

~~~{.r}
### 모형 정확성
plot(history$metrics$acc, main="Model Accuracy", xlab = "epoch", ylab="accuracy", col="blue", type="l",
     ylim=c(0,1))
lines(history$metrics$val_acc, col="green")
legend("bottomright", c("train","test"), col=c("blue", "green"), lty=c(1,1))
~~~

<img src="fig/keras-iris-deep-learning-fit-3.png" style="display: block; margin: auto;" />

## 6. 딥러닝 모형 적합성 시각화

붓꽃 분류기가 개발되었으니, 실제 성능이 얼마나 나오는지 앞서 검증표본으로 남겨 놓은 `iris.test` 데이터를 통해 
정확도를 추정한다.


~~~{.r}
### 3.2.7. 검증표본을 통한 평가
pred_mat <- model %>% predict(iris.test, batch_size = 5) %>% tbl_df()

pred_mat <- pred_mat %>% 
    mutate(max_prob = max.col(., ties.method = "last")-1)

### 오차 행렬(Confusion Matrix)
table(iris.testtarget, pred_mat$max_prob)
~~~



~~~{.output}
               
iris.testtarget  0  1  2
              0 11  0  0
              1  0 16  0
              2  0  1 14

~~~



~~~{.r}
### 모형 정확도 평가
score <- model %>% 
    evaluate(iris.test, iris.testLabels, batch_size = 128)

print(score)
~~~



~~~{.output}
[[1]]
[1] 0.1808525

[[2]]
[1] 0.9761904

~~~


